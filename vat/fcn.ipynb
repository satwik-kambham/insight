{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Segmentation using FCN on Pascal VOC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install lightning albumentations torchinfo torchmetrics wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import wandb\n",
    "# wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import random_split, DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "import lightning.pytorch as pl\n",
    "\n",
    "import albumentations as A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AugmentedDataset:\n",
    "    def __init__(\n",
    "        self,\n",
    "        dataset,\n",
    "        augmentation=None,\n",
    "        img_transforms=None,\n",
    "        mask_transforms=None,\n",
    "    ):\n",
    "        self.dataset = dataset\n",
    "        self.augmentation = augmentation\n",
    "        self.img_transforms = img_transforms\n",
    "        self.mask_transforms = mask_transforms\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img, mask = self.dataset[idx]\n",
    "\n",
    "        img = np.array(img)\n",
    "        mask = np.array(mask)\n",
    "\n",
    "        if self.augmentation:\n",
    "            augmented = self.augmentation(image=img, mask=mask)\n",
    "            img = augmented[\"image\"]\n",
    "            mask = augmented[\"mask\"]\n",
    "\n",
    "        if self.img_transforms:\n",
    "            img = self.img_transforms(img)\n",
    "\n",
    "        if self.mask_transforms:\n",
    "            mask = self.mask_transforms(mask)\n",
    "\n",
    "        return img, mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_augmentation(img_shape):\n",
    "    train_transform = A.Compose(\n",
    "        [\n",
    "            A.Resize(*img_shape),\n",
    "            A.Rotate(),\n",
    "            A.HorizontalFlip(),\n",
    "            A.RGBShift(),\n",
    "            A.Blur(),\n",
    "            A.RandomBrightnessContrast(),\n",
    "            A.CLAHE(),\n",
    "            A.Resize(*img_shape),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    val_transform = A.Compose(\n",
    "        [\n",
    "            A.Resize(*img_shape),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    return train_transform, val_transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_VOCSegmentationDataset(data_dir, img_shape=(512, 512)):\n",
    "    img_transforms = transforms.Compose(\n",
    "        [\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    def mask_to_tensor(mask):\n",
    "        mask = np.array(mask)\n",
    "        mask[mask == 255] = 0 # Ignore the void class\n",
    "        tensor_mask = torch.from_numpy(mask).long()\n",
    "        return tensor_mask\n",
    "\n",
    "    mask_transforms = transforms.Compose(\n",
    "        [\n",
    "            transforms.Lambda(lambda x: mask_to_tensor(x)),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    dataset = datasets.VOCSegmentation(\n",
    "        root=data_dir,\n",
    "        image_set=\"trainval\",\n",
    "        download=True,\n",
    "    )\n",
    "\n",
    "    train_dataset, val_dataset = random_split(dataset, [0.9, 0.1])\n",
    "    train_augmentation, val_augmentation = get_augmentation(img_shape)\n",
    "    train_dataset = AugmentedDataset(\n",
    "        train_dataset, train_augmentation, img_transforms, mask_transforms\n",
    "    )\n",
    "    val_dataset = AugmentedDataset(\n",
    "        val_dataset, val_augmentation, img_transforms, mask_transforms\n",
    "    )\n",
    "\n",
    "    num_classes = 21\n",
    "\n",
    "    return train_dataset, val_dataset, num_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset, val_dataset, num_classes = load_VOCSegmentationDataset(\n",
    "    \"datasets\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check shapes and visualize\n",
    "print(train_dataset[0][0].shape, train_dataset[0][1].shape, train_dataset[0][0].min(), train_dataset[0][0].max())\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def visualize_dataset(dataset, num_samples=5):\n",
    "    fig, axs = plt.subplots(2, num_samples, figsize=(15, 5))\n",
    "    for i in range(num_samples):\n",
    "        img, mask = dataset[i]\n",
    "        img = img.numpy().transpose(1, 2, 0)\n",
    "        # Img in range -1 to 1, so we need to rescale to 0 to 1\n",
    "        img = (img + 1) / 2\n",
    "        axs[0, i].imshow(img)\n",
    "        axs[1, i].imshow(mask)\n",
    "        print(mask.unique())\n",
    "    plt.show()\n",
    "\n",
    "visualize_dataset(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SegmentationDataModule(pl.LightningDataModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        data_dir,\n",
    "        inp_size=512,\n",
    "        batch_size=1,\n",
    "        num_workers=0,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.data_dir = data_dir\n",
    "        self.inp_size = inp_size\n",
    "        self.batch_size = batch_size\n",
    "        self.num_workers = num_workers\n",
    "\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "    def setup(self, stage):\n",
    "        self.train_dataset, self.val_dataset, _ = load_VOCSegmentationDataset(\n",
    "            self.data_dir,\n",
    "            img_shape=(self.inp_size, self.inp_size),\n",
    "        )\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.train_dataset,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=True,\n",
    "            drop_last=True,\n",
    "            num_workers=self.num_workers,\n",
    "        )\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.val_dataset,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=False,\n",
    "            drop_last=True,\n",
    "            num_workers=self.num_workers,\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.models as models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check backbone architecture\n",
    "# resnet = models.resnet18(pretrained=True)\n",
    "# resnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FCN(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super().__init__()\n",
    "\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "        # backbone = models.resnet50(weights=models.ResNet50_Weights.IMAGENET1K_V1)\n",
    "        # backbone = models.resnet50(pretrained=True)\n",
    "        backbone = models.vgg16(pretrained=True)\n",
    "\n",
    "        # self.backbone = nn.Sequential(\n",
    "        #     *list(backbone.children())[:-2],\n",
    "        # )\n",
    "        self.backbone = nn.Sequential(\n",
    "            *list(backbone.features.children())\n",
    "        )\n",
    "\n",
    "        self.classifier = nn.LazyConv2d(num_classes, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.backbone(x)\n",
    "        x = self.classifier(x)\n",
    "        x = F.interpolate(x, size=(512, 512), mode=\"bilinear\", antialias=True)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check output shape\n",
    "# fcn = FCN_ResNet18(num_classes=21)\n",
    "# test_input = torch.randn(1, 3, 512, 512)\n",
    "# test_output = fcn(test_input)\n",
    "# test_output.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "from lightning.pytorch.loggers import TensorBoardLogger, WandbLogger\n",
    "\n",
    "from torchinfo import summary\n",
    "\n",
    "import torchmetrics as tm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import matplotlib\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "def generate_mask(pred, num_classes, one_hot=True):\n",
    "    if one_hot:\n",
    "        pred_labels = pred.argmax(dim=1, keepdim=False).cpu().numpy()\n",
    "    else:\n",
    "        pred_labels = pred.cpu().numpy()\n",
    "\n",
    "    # Create color map given number of classes\n",
    "    color_map = matplotlib.colormaps.get_cmap(\"gnuplot2\")\n",
    "    color_map = matplotlib.colors.ListedColormap(\n",
    "        color_map(np.linspace(0, 1, num_classes))\n",
    "    )\n",
    "\n",
    "    rgb_mask = color_map(pred_labels[0] / num_classes)\n",
    "    rgb_mask = (rgb_mask[:, :, :3] * 255).astype(np.uint8)\n",
    "    rgb_image = Image.fromarray(rgb_mask)\n",
    "\n",
    "    return rgb_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FCNModule(pl.LightningModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        inp_size=512,\n",
    "        num_classes=21,\n",
    "        lr=1e-4,\n",
    "        momentum=0.9,\n",
    "        weight_decay=0.0625,\n",
    "        compile=False,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "        self.model = FCN(num_classes)\n",
    "\n",
    "        test_input_shape = (1, 3, inp_size, inp_size)\n",
    "        test_input = torch.randn(test_input_shape)\n",
    "        _ = self.model(test_input)\n",
    "\n",
    "        print(summary(self.model, input_size=test_input_shape))\n",
    "\n",
    "        if compile:\n",
    "            self.model = self.model.compile()\n",
    "\n",
    "        self.lr = lr\n",
    "        self.momentum = momentum\n",
    "        self.weight_decay = weight_decay\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "        self.accuracy = tm.Accuracy(task=\"multiclass\", num_classes=num_classes)\n",
    "        self.iou = tm.JaccardIndex(task=\"multiclass\", num_classes=num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        data, target = batch\n",
    "        output = self(data)\n",
    "        loss = self.criterion(output, target)\n",
    "        self.log(\"loss\", loss)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        data, target = batch\n",
    "        output = self(data)\n",
    "        loss = self.criterion(output, target)\n",
    "        self.log(\"val_loss\", loss)\n",
    "\n",
    "        pred = output.argmax(dim=1, keepdim=False)\n",
    "\n",
    "        self.val_pred = output\n",
    "        self.val_target = target\n",
    "\n",
    "        self.accuracy(pred, target)\n",
    "        self.log(\"val_acc\", self.accuracy, on_step=False, on_epoch=True)\n",
    "\n",
    "        self.iou(pred, target)\n",
    "        self.log(\"val_iou\", self.iou, on_step=False, on_epoch=True)\n",
    "\n",
    "    def on_validation_epoch_end(self):\n",
    "        if self.num_classes == 1:\n",
    "            self.val_pred = torch.sigmoid(self.val_pred)\n",
    "            mask_image = generate_mask(self.val_pred, self.num_classes + 1, False)\n",
    "        else:\n",
    "            mask_image = generate_mask(self.val_pred, self.num_classes + 1)\n",
    "        target_mask = generate_mask(self.val_target, self.num_classes + 1, False)\n",
    "\n",
    "        for logger in self.loggers:\n",
    "            if isinstance(logger, TensorBoardLogger):\n",
    "                np_mask_image = np.array(mask_image.convert(\"RGB\"))\n",
    "                np_mask_image = np_mask_image.transpose(2, 0, 1)\n",
    "                logger.experiment.add_image(\n",
    "                    \"val_mask\", np_mask_image, global_step=self.current_epoch\n",
    "                )\n",
    "\n",
    "            if isinstance(logger, WandbLogger):\n",
    "                logger.log_image(key=\"val_mask\", images=[mask_image])\n",
    "                logger.log_image(key=\"val_target\", images=[target_mask])\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = optim.SGD(\n",
    "            self.parameters(),\n",
    "            lr=self.lr,\n",
    "            momentum=self.momentum,\n",
    "            weight_decay=self.weight_decay,\n",
    "        )\n",
    "        return optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightning.pytorch.callbacks import LearningRateMonitor\n",
    "from lightning.pytorch.tuner import Tuner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datamodule = SegmentationDataModule(\n",
    "    \"datasets\",\n",
    ")\n",
    "fcn_module = FCNModule()\n",
    "wandb_logger = WandbLogger(project=\"semantic_segmentation\")\n",
    "tensorboard_logger = TensorBoardLogger(\"tensorboard_logs/\")\n",
    "lr_monitor = LearningRateMonitor(log_momentum=True)\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    accelerator=\"cpu\",\n",
    "    max_epochs=100,\n",
    "    logger=[wandb_logger, tensorboard_logger],\n",
    "    callbacks=[lr_monitor],\n",
    "    log_every_n_steps=10,\n",
    "    # fast_dev_run=True,\n",
    "    accumulate_grad_batches=20,\n",
    ")\n",
    "\n",
    "tuner = Tuner(trainer)\n",
    "lr_finder = tuner.lr_find(fcn_module, datamodule=datamodule)\n",
    "print(lr_finder.results)\n",
    "print(lr_finder.suggestion())\n",
    "\n",
    "trainer.fit(fcn_module, datamodule=datamodule)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference / Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test model on some images from the validation dataset and visualize\n",
    "def visualize_predictions(model, dataset, num_samples=5):\n",
    "    fig, axs = plt.subplots(3, num_samples, figsize=(15, 5))\n",
    "    for i in range(num_samples):\n",
    "        img, mask = dataset[i]\n",
    "        img = img.unsqueeze(0)\n",
    "        pred = model(img)\n",
    "        pred = pred.argmax(dim=1, keepdim=False)\n",
    "        pred = pred.squeeze(0)\n",
    "        pred = pred.numpy()\n",
    "        axs[0, i].imshow(img.squeeze(0).numpy().transpose(1, 2, 0))\n",
    "        axs[1, i].imshow(mask.numpy())\n",
    "        axs[2, i].imshow(pred)\n",
    "    plt.show()\n",
    "\n",
    "visualize_predictions(fcn_module, val_dataset)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "insight",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
